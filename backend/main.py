from fastapi import FastAPI, File, UploadFile, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import tempfile
import os
import subprocess
import openai
from datetime import datetime

print("‚úÖ API DO OUVIESCREVI INICIADA")
print("üîë Chave carregada:", bool(os.getenv("OPENAI_API_KEY")))


app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

MAX_FILE_SIZE_MB = 25
SEGMENT_DURATION = 600  # segundos (10 minutos)

def format_segments(segments):
    def format_time(seconds):
        m, s = divmod(int(seconds), 60)
        return f"[{m:02d}:{s:02d}]"
    formatted_text = ""
    for s in segments:
        timestamp = format_time(s.start)
        formatted_text += f"{timestamp} {s.text.strip()}\n\n"
    return formatted_text.strip()

def split_audio(input_path, output_dir, segment_duration=SEGMENT_DURATION):
    os.makedirs(output_dir, exist_ok=True)
    command = [
        "ffmpeg",
        "-i", input_path,
        "-f", "segment",
        "-segment_time", str(segment_duration),
        "-c", "copy",
        os.path.join(output_dir, "segment_%03d.wav"),
        "-y"
    ]
    subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
    return sorted([os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(".wav")])

@app.post("/transcribe")
async def transcribe(file: UploadFile = File(...)):
    print(f"[{datetime.now()}] Upload recebido: {file.filename}")


    contents = await file.read()

    if len(contents) > MAX_FILE_SIZE_MB * 1024 * 1024:
        return {"error": f"Ficheiro demasiado grande. Limite: {MAX_FILE_SIZE_MB}MB"}

    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp.write(contents)
        tmp_path = tmp.name

    audio_path = tmp_path + ".wav"

    try:
        subprocess.run([
            "ffmpeg", "-i", tmp_path,
            "-ar", "16000", "-ac", "1",
            "-c:a", "pcm_s16le", audio_path, "-y"
        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
    except Exception as e:
        return {"error": f"Erro ao converter √°udio: {str(e)}"}

    split_dir = tempfile.mkdtemp()
    try:
        parts = split_audio(audio_path, split_dir)
        full_text, formatted_text = "", ""

        for part in parts:
            with open(part, "rb") as audio:
                result = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio,
                    response_format="verbose_json"
                )
            full_text += result.text + "\n"
            formatted_text += format_segments(result.segments) + "\n\n"

        return {
            "transcription": full_text.strip(),
            "formatted": formatted_text.strip()
        }
    except Exception as e:
        return {"error": f"Erro ao processar ficheiro: {str(e)}"}
    finally:
        os.remove(tmp_path)
        os.remove(audio_path)
        for f in os.listdir(split_dir):
            os.remove(os.path.join(split_dir, f))
        os.rmdir(split_dir)

class SummarizeRequest(BaseModel):
    text: str
    token: str = ""
    mode: str = "normal"

@app.post("/summarize")
async def summarize(req: SummarizeRequest):
    if req.token != os.getenv("ADMIN_TOKEN", "ouviescrevi2025@resumo"):
        return {"error": "Token inv√°lido ou ausente."}

    if req.mode == "minuta":
        prompt = (
            "A partir da seguinte transcri√ß√£o de uma reuni√£o ou conversa, gera uma minuta clara e organizada "
            "em formato de t√≥picos. Inclui:\n"
            "- T√≥picos discutidos\n- Decis√µes tomadas\n- Respons√°veis (se mencionados)\n- A√ß√µes a realizar\n\n"
            f"Transcri√ß√£o:\n{req.text}"
        )
    else:
        prompt = f"Resume de forma clara e concisa a seguinte transcri√ß√£o:\n\n{req.text}"

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "√âs um assistente que resume transcri√ß√µes de √°udio."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=400
        )
        return {"summary": response.choices[0].message.content.strip()}
    except Exception as e:
        return {"error": str(e)}

@app.post("/translate")
async def translate_text(request: Request):
    data = await request.json()
    text = data.get("text")
    language = data.get("language")
    token = data.get("token")

    if token != "ouviescrevi2025@resumo":
        raise HTTPException(status_code=403, detail="Token inv√°lido.")

    idiomas_suportados = ["ingl√™s", "espanhol", "franc√™s", "alem√£o", "italiano", "portugu√™s"]
    if language.lower() not in idiomas_suportados:
        return {"error": f"Idioma n√£o suportado: {language}"}

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": f"Traduz o texto para {language}."},
                {"role": "user", "content": text}
            ],
            temperature=0.3
        )
        return {"translation": response.choices[0].message.content.strip()}
    except Exception as e:
        return {"error": str(e)}

class ClassifyRequest(BaseModel):
    text: str
    token: str

@app.post("/classify")
async def classify_content(request: ClassifyRequest):
    if request.token != "ouviescrevi2025@resumo":
        return {"error": "Token inv√°lido"}

    prompt = (
        "Classifica o tipo de conte√∫do abaixo como uma das seguintes op√ß√µes:\n"
        "- Entrevista\n- Aula\n- Podcast\n- Reuni√£o\n- Apresenta√ß√£o\n- Testemunho\n- Conversa informal\n\n"
        f"Texto:\n{request.text}\n\nResponde s√≥ com o tipo mais prov√°vel."
    )

    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=20
        )
        return {"type": response.choices[0].message.content.strip()}
    except Exception as e:
        return {"error": str(e)}

@app.post("/correct")
async def correct_text(req: Request):
    data = await req.json()
    text = data.get("text", "")
    token = data.get("token")

    if token != "ouviescrevi2025@resumo":
        return {"error": "Token inv√°lido"}

    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "Corrige ortografia e gram√°tica do texto. Mant√©m o conte√∫do original."},
                {"role": "user", "content": text}
            ]
        )
        return {"corrected": response.choices[0].message.content.strip()}
    except Exception as e:
        return {"error": str(e)}


class EmailRequest(BaseModel):
    text: str
    token: str
    tone: str = "formal"

@app.post("/generate-email")
async def generate_email(req: EmailRequest):
    if req.token != os.getenv("ADMIN_TOKEN", "ouviescrevi2025@resumo"):
        return {"error": "Token inv√°lido"}

    prompt = (
        f"Gera um email em tom {req.tone} a partir da seguinte transcri√ß√£o ou resumo de uma reuni√£o:\n\n"
        f"{req.text}\n\n"
        "O email deve ser claro, direto e adequado para enviar ap√≥s a reuni√£o."
    )

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "√âs um assistente que escreve e-mails profissionais a partir de resumos ou transcri√ß√µes."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500
        )
        return {"email": response.choices[0].message.content.strip()}
    except Exception as e:
        return {"error": str(e)}
